# BlackRoad AI Cluster - Planning

> Development planning for GPU cluster orchestration

## Vision

Build an intelligent GPU cluster management system with:
- Automatic workload scheduling
- Cost optimization
- Multi-cloud GPU provisioning
- Real-time monitoring

---

## Cluster Inventory

### Current Resources

| Provider | GPU Type | Count | Memory | Status |
|----------|----------|-------|--------|--------|
| Railway | A100 | 1 | 80GB | âœ… Active |
| Local | RTX 4090 | 1 | 24GB | âœ… Active |
| - | - | - | - | - |

### Target Resources (Q2 2026)

| Provider | GPU Type | Count | Memory | Purpose |
|----------|----------|-------|--------|---------|
| Railway | H100 | 8 | 640GB | Production |
| Railway | A100 | 4 | 320GB | Staging |
| Local | RTX 4090 | 2 | 48GB | Development |
| Spot | Mixed | 10+ | Varies | Burst |

---

## Current Sprint

### Sprint 2026-02

#### Goals
- [ ] Design cluster scheduler
- [ ] Implement GPU health monitoring
- [ ] Create cost optimization rules
- [ ] Build admin dashboard

#### Tasks

| Task | Priority | Status | Est. |
|------|----------|--------|------|
| Scheduler architecture | P0 | ğŸ”„ In Progress | 3d |
| Health check system | P0 | ğŸ“‹ Planned | 2d |
| Cost tracking | P1 | ğŸ“‹ Planned | 2d |
| Dashboard UI | P2 | ğŸ“‹ Planned | 4d |

---

## Scheduler Design

### Job Queue

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    JOB SCHEDULER                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚                    Job Queue                          â”‚ â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚ â”‚
â”‚  â”‚  â”‚ Job 1  â”‚ â”‚ Job 2  â”‚ â”‚ Job 3  â”‚ â”‚ Job N  â”‚        â”‚ â”‚
â”‚  â”‚  â”‚ P: Hi  â”‚ â”‚ P: Med â”‚ â”‚ P: Low â”‚ â”‚ P: Med â”‚        â”‚ â”‚
â”‚  â”‚  â”‚ GPU: 4 â”‚ â”‚ GPU: 1 â”‚ â”‚ GPU: 2 â”‚ â”‚ GPU: 8 â”‚        â”‚ â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                           â”‚                                 â”‚
â”‚                           â–¼                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚                   Scheduler                           â”‚ â”‚
â”‚  â”‚                                                       â”‚ â”‚
â”‚  â”‚  Rules:                                               â”‚ â”‚
â”‚  â”‚  1. Priority: Hi > Med > Low                         â”‚ â”‚
â”‚  â”‚  2. GPU fit: Find smallest fit                       â”‚ â”‚
â”‚  â”‚  3. Locality: Prefer same node                       â”‚ â”‚
â”‚  â”‚  4. Cost: Use spot for low priority                  â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                           â”‚                                 â”‚
â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚         â–¼                 â–¼                 â–¼              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚  Node 1     â”‚   â”‚  Node 2     â”‚   â”‚  Node 3     â”‚      â”‚
â”‚  â”‚  H100 x4    â”‚   â”‚  H100 x4    â”‚   â”‚  A100 x4    â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Scheduling Algorithm

```python
def schedule_job(job):
    # 1. Find eligible nodes
    eligible = [n for n in nodes if n.available_gpus >= job.gpu_count]

    # 2. Apply scheduling rules
    if job.priority == "high":
        # Use dedicated nodes
        eligible = [n for n in eligible if n.type == "dedicated"]
    elif job.priority == "low":
        # Prefer spot instances
        eligible = sorted(eligible, key=lambda n: n.cost_per_gpu)

    # 3. Select best fit
    node = min(eligible, key=lambda n: n.available_gpus - job.gpu_count)

    # 4. Schedule
    return node.schedule(job)
```

---

## Cost Optimization

### Pricing (per hour)

| GPU | Dedicated | Spot | Savings |
|-----|-----------|------|---------|
| H100 | $4.00 | $1.20 | 70% |
| A100 | $2.50 | $0.75 | 70% |
| A10 | $1.00 | $0.30 | 70% |

### Optimization Strategies

1. **Spot Instance Usage**
   - Use for batch jobs
   - Implement checkpointing
   - Auto-migrate on preemption

2. **Auto-Scaling**
   - Scale down during low usage
   - Pre-warm before peak hours
   - Right-size instances

3. **Job Consolidation**
   - Batch similar jobs
   - Multi-tenant GPU sharing
   - Time-based scheduling

### Monthly Cost Targets

| Month | Current | Target | Savings |
|-------|---------|--------|---------|
| Feb 2026 | $2,000 | $2,000 | - |
| Mar 2026 | $5,000 | $4,000 | 20% |
| Apr 2026 | $10,000 | $7,500 | 25% |
| May 2026 | $15,000 | $10,500 | 30% |

---

## Health Monitoring

### Metrics Collected

| Metric | Interval | Alert |
|--------|----------|-------|
| GPU utilization | 10s | <20% |
| GPU memory | 10s | >95% |
| GPU temperature | 30s | >85Â°C |
| Job duration | On complete | >SLA |
| Error rate | 1m | >1% |

### Health Dashboard

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   GPU CLUSTER HEALTH                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  NODE           GPU    UTIL   MEM    TEMP   JOBS   STATUS  â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚
â”‚  railway-h100-1  H100   87%   72GB   68Â°C    3    â— Online â”‚
â”‚  railway-h100-2  H100   92%   75GB   71Â°C    4    â— Online â”‚
â”‚  railway-a100-1  A100   45%   32GB   55Â°C    1    â— Online â”‚
â”‚  local-4090-1    4090   0%    0GB    42Â°C    0    â—‹ Idle   â”‚
â”‚                                                             â”‚
â”‚  CLUSTER TOTALS                                            â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                             â”‚
â”‚  Total GPUs: 12      Active Jobs: 8                        â”‚
â”‚  Total Memory: 464GB  Queue Depth: 15                      â”‚
â”‚  Avg Utilization: 74%  Est. Wait: 12min                    â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## API Design

### Endpoints

```
POST /jobs              # Submit job
GET  /jobs              # List jobs
GET  /jobs/:id          # Job details
DELETE /jobs/:id        # Cancel job

GET  /nodes             # List nodes
GET  /nodes/:id         # Node details
POST /nodes/:id/drain   # Drain node

GET  /cluster/status    # Cluster overview
GET  /cluster/costs     # Cost breakdown
POST /cluster/scale     # Scale cluster
```

### Job Spec

```yaml
job:
  name: "llama-70b-inference"
  image: "blackroad/vllm:latest"
  gpu_count: 4
  gpu_type: "H100"
  priority: "high"
  timeout: 3600
  env:
    MODEL: "meta-llama/Llama-2-70b"
  resources:
    memory: "256Gi"
    cpu: "32"
```

---

*Last updated: 2026-02-05*
